{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N8A5UYiE4L7"
      },
      "source": [
        "# CENG403 - Spring 2024 - THE3\n",
        "In this THE, you will first implement convolution & pooling operations of a CNN and their backpropagation(Task 1). Then, you will experience how CNNs can be implemented in PyTorch(Task2). Finally, you will finetune a ResNet18 (prerained on ImageNet) on CIFAR10 (Task3).\n",
        "\n",
        "Note that these Colab files are provided read-only and you should copy them to your Google Drives/Colab folders to be able to save your changes.\n",
        "\n",
        "Once you are done working on your files, without clearing the outputs, save them on your computer and zip them into a file THE3.zip without placing them into a directory. Upload this ZIP file on the ODTUClass page of the course under assignment \"THE3\".\n",
        "# Task 1: Naive Convolution and Pooling\n",
        "In this task, you will implement convolution and pooling operations in a naive way without using any matrix-vector multiplication.\n",
        "\n",
        "*Disclaimer: Some components are adapted or taken from [CS231n](https://cs231n.github.io/) materials.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfmBLQj5OAnT"
      },
      "source": [
        "## 1.1 Import the Modules\n",
        "\n",
        "As usual, let us import the modules that we will use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSIkrblIOA_R"
      },
      "source": [
        "import matplotlib.pyplot as plt # For plotting\n",
        "import numpy as np              # NumPy, for working with arrays/tensors\n",
        "import os                       # Built-in library for filesystem access etc.\n",
        "import pickle                   # For (re)storing Python objects into (from) files\n",
        "import time                     # For measuring time\n",
        "import random                   # Python's random library\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 100 # 200 e.g. is really fine, but slower"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6NQtyenE8kd"
      },
      "source": [
        "## 1.2 Naive Implementation of Convolution\n",
        "\n",
        "Here, we will implement 2D convolution using only for/while loops; in other words, **NO** vector/matrix multiplications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF-mlexaVVAf"
      },
      "source": [
        "### 1.2.1 Convolution Feedforward\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-8UbQIsJkIj"
      },
      "source": [
        "def conv_forward_naive(x, w, b, stride, padding):\n",
        "    \"\"\"\n",
        "    The input consists of N samples, each with C channels, height H and\n",
        "    width W. We convolve each input with F different filters, where each filter\n",
        "    spans all C channels and has height FH and width FW.\n",
        "\n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, FH, FW)\n",
        "    - b: Biases, of shape (F,)\n",
        "    - stride: The number of pixels between adjacent receptive fields in the\n",
        "        horizontal and vertical directions.\n",
        "    - padding: The number of pixels that will be used to zero-pad the input.\n",
        "\n",
        "    During padding, 'padding'-many zeros should be placed symmetrically (i.e equally on both sides)\n",
        "    along the height and width axes of the input. Be careful not to modfiy the original\n",
        "    input x directly.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H + 2 * padding - FH) / stride\n",
        "      W' = 1 + (W + 2 * padding - FW) / stride\n",
        "    - cache: (x_pad, w, b, stride, padding)\n",
        "    \"\"\"\n",
        "\n",
        "    # Layer output:\n",
        "    out = None\n",
        "\n",
        "    # Get values for the dimensions:\n",
        "    #sample, channel, height,width\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    #filters, channels, height, width\n",
        "    F, C, FH, FW = w.shape\n",
        "\n",
        "    # Just to make sure\n",
        "    if (H - FH + 2 * padding) % stride != 0:\n",
        "      return ValueError(\"Filters do not align horizontally\")\n",
        "    if (W - FW + 2 * padding) % stride != 0:\n",
        "      return ValueError(\"Filters do not align vertically\")\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional forward pass. You cannot use          #\n",
        "    # vector or matrix multiplications here. Be careful about stride.         #\n",
        "    # Hints:                                                                  #\n",
        "    #  (1) You can use the function np.pad for padding.                       #\n",
        "    #  (2) You will have such a nested loop structure here:                   #\n",
        "    #    - loop over N samples                                                #\n",
        "    #      - loop over F filters                                              #\n",
        "    #        - loop over rows of x_pad                                        #\n",
        "    #          - loop over cols of x_pad                                      #\n",
        "    #            - loop over kernel rows                                      #\n",
        "    #              - loop over kernel cols                                    #\n",
        "    #                - loop over channels                                     #\n",
        "    ###########################################################################\n",
        "\n",
        "    # output dimension calculation\n",
        "    # H_out = W_out\n",
        "    # H_out = 1 + (height + 2*padding - filt. height) / stride -- integer division\n",
        "    H_out = 1 + (H + 2 * padding - FH) // stride\n",
        "    W_out = 1 + (W + 2 * padding - FW) // stride\n",
        "\n",
        "    # output array initialized with zeros:\n",
        "    out = np.zeros((N, F, H_out, W_out))\n",
        "\n",
        "    # add zero padding to input -- dimension\n",
        "    # numpy.pad(array, pad_width, mode='constant', **kwargs)\n",
        "    # pad_width{sequence, array_like, int}\n",
        "    # pad width : (0,0), (0,0), (padding,padding), (padding,padding)\n",
        "    x_pad = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='constant')\n",
        "\n",
        "    # convolution:\n",
        "    for n in range(N):  #  N samples\n",
        "        for f in range(F):  #  F filters\n",
        "            for h_out in range(H_out):  #  output height\n",
        "                for w_out in range(W_out):  #  output width\n",
        "                    convol = 0\n",
        "                    for c in range(C):  # input channels\n",
        "                        for fh in range(FH):  # filter height\n",
        "                            for fw in range(FW):  # filter width\n",
        "                                h_start = h_out * stride + fh\n",
        "                                w_start = w_out * stride + fw\n",
        "                                convol += x_pad[n, c, h_start, w_start] * w[f, c, fh, fw]\n",
        "                    convol += b[f]\n",
        "                    out[n, f, h_out, w_out] = convol\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x_pad, w, b, stride, padding)\n",
        "    return out, cache"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQk5fjckWhnR"
      },
      "source": [
        "### 1.2.2 Convolution Feedforward Check\n",
        "\n",
        "Let us quickly check your implementation with some known values. Your output should differ by $\\sim 10^{-8}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWO4tshRIAxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a523b0c6-a75a-4060-abd0-027c626e8017"
      },
      "source": [
        "\n",
        "x_shape = (2, 3, 4, 4)\n",
        "w_shape = (3, 3, 4, 4)\n",
        "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
        "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
        "b = np.linspace(-0.1, 0.2, num=3)\n",
        "\n",
        "stride = 2\n",
        "padding = 1\n",
        "out, _ = conv_forward_naive(x, w, b, stride, padding)\n",
        "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
        "                           [-0.18387192, -0.2109216 ]],\n",
        "                          [[ 0.21027089,  0.21661097],\n",
        "                           [ 0.22847626,  0.23004637]],\n",
        "                          [[ 0.50813986,  0.54309974],\n",
        "                           [ 0.64082444,  0.67101435]]],\n",
        "                         [[[-0.98053589, -1.03143541],\n",
        "                           [-1.19128892, -1.24695841]],\n",
        "                          [[ 0.69108355,  0.66880383],\n",
        "                           [ 0.59480972,  0.56776003]],\n",
        "                          [[ 2.36270298,  2.36904306],\n",
        "                           [ 2.38090835,  2.38247847]]]])\n",
        "\n",
        "def rel_error(x, y):\n",
        "  \"\"\" returns relative error \"\"\"\n",
        "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
        "\n",
        "# Compare your output to ours; difference should be around e-8\n",
        "print('Testing conv_forward_naive')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing conv_forward_naive\n",
            "difference:  2.2121476575931688e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4DXOOvBWSjE"
      },
      "source": [
        "### 1.2.3 Convolution Gradient\n",
        "\n",
        "Now, let us implement the gradient. Again, you are **NOT** allowed to use any vector/matrix multiplication here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ewZb1xkroZX"
      },
      "source": [
        "def conv_backward_naive(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a convolutional layer.\n",
        "\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives.\n",
        "    - cache: A tuple of (x, w, b, stride, padding) as in conv_forward_naive\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x.\n",
        "    - dw: Gradient with respect to w\n",
        "    - db: Gradient with respect to b\n",
        "    \"\"\"\n",
        "    dx, dw, db = None, None, None\n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional backward pass.                        #\n",
        "    ###########################################################################\n",
        "\n",
        "    x_pad, w, b, stride, padding = cache\n",
        "    N, C, H_pad, W_pad = x_pad.shape\n",
        "    F, _, FH, FW = w.shape\n",
        "    _, _, H_out, W_out = dout.shape\n",
        "\n",
        "    # gradient initialization:\n",
        "    dx_pad = np.zeros_like(x_pad)\n",
        "    dw = np.zeros_like(w)\n",
        "    db = np.zeros_like(b)\n",
        "\n",
        "    # db:\n",
        "    for f in range(F):\n",
        "        db[f] = np.sum(dout[:, f, :, :])\n",
        "\n",
        "    # dw and dx_pad\n",
        "    for n in range(N):  # N samples\n",
        "        for f in range(F):  # F filters\n",
        "            for h_out in range(H_out):  # output height\n",
        "                for w_out in range(W_out):  # output width\n",
        "                    initial_h = h_out * stride\n",
        "                    initial_w = w_out * stride\n",
        "                    h_end = initial_h + FH\n",
        "                    w_end = initial_w + FW\n",
        "\n",
        "                    # gradient for dw\n",
        "                    for c in range(C):  # channels\n",
        "                        for fh in range(FH):  # filter height\n",
        "                            for fw in range(FW):  # filter width\n",
        "                                dw[f, c, fh, fw] += x_pad[n, c, initial_h + fh, initial_w + fw] * dout[n, f, h_out, w_out]\n",
        "\n",
        "                    # gradient for dx_pad\n",
        "                    for c in range(C):  # channels\n",
        "                        for fh in range(FH):  # filter height\n",
        "                            for fw in range(FW):  # filter width\n",
        "                                dx_pad[n, c, initial_h + fh, initial_w + fw] += w[f, c, fh, fw] * dout[n, f, h_out, w_out]\n",
        "\n",
        "    # padding removal\n",
        "    if padding != 0:\n",
        "        dx = dx_pad[:, :, padding:-padding, padding:-padding]\n",
        "    else:\n",
        "        dx = dx_pad\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka7ev5-BwTSc"
      },
      "source": [
        "### 1.2.4 Convolution Gradient Check\n",
        "\n",
        "Let us check the gradient. You should see differences lower than $10^{-9}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSk5T7cOwUtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12f866b-fa12-4252-9de1-e6f69991255a"
      },
      "source": [
        "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
        "    \"\"\"\n",
        "    Evaluate a numeric gradient for a function that accepts a numpy\n",
        "    array and returns a numpy array.\n",
        "    \"\"\"\n",
        "    grad = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        ix = it.multi_index\n",
        "\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h\n",
        "        pos = f(x).copy()\n",
        "        x[ix] = oldval - h\n",
        "        neg = f(x).copy()\n",
        "        x[ix] = oldval\n",
        "\n",
        "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
        "        it.iternext()\n",
        "    return grad\n",
        "\n",
        "np.random.seed(231)\n",
        "x = np.random.randn(4, 3, 5, 5)\n",
        "w = np.random.randn(2, 3, 3, 3)\n",
        "b = np.random.randn(2,)\n",
        "dout = np.random.randn(4, 2, 5, 5)\n",
        "stride = 1\n",
        "pad = 1\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, stride, pad)[0], x, dout)\n",
        "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, stride, pad)[0], w, dout)\n",
        "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, stride, pad)[0], b, dout)\n",
        "\n",
        "out, cache = conv_forward_naive(x, w, b, stride, pad)\n",
        "dx, dw, db = conv_backward_naive(dout, cache)\n",
        "\n",
        "# Your errors should be around e-8 or less.\n",
        "print('Testing conv_backward_naive function')\n",
        "print('dx error: ', rel_error(dx, dx_num))\n",
        "print('dw error: ', rel_error(dw, dw_num))\n",
        "print('db error: ', rel_error(db, db_num))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing conv_backward_naive function\n",
            "dx error:  1.5742819260583776e-08\n",
            "dw error:  1.794673097886394e-10\n",
            "db error:  2.584845543531467e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj-avor_VY-O"
      },
      "source": [
        "## 1.3 Pooling\n",
        "\n",
        "Now, we implement pooling, max-pooling to be specific. Again, **no** use of vector/matrix multiplications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyEDErauzUuw"
      },
      "source": [
        "### 1.3.1 Pooling Feedforward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6SOEQieWbh8"
      },
      "source": [
        "def max_pool_forward_naive(x, stride, PH, PW):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a max-pooling layer.\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C, H, W)\n",
        "    - stride: The distance between adjacent pooling regions\n",
        "    - PH: The height of each pooling region\n",
        "    - PW: The width of each pooling region\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H - pool_height) / stride\n",
        "      W' = 1 + (W - pool_width) / stride\n",
        "    - cache: (x, pool_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the max-pooling forward pass                            #\n",
        "    ###########################################################################\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    # output dimensions\n",
        "    H_out = 1 + (H - PH) // stride\n",
        "    W_out = 1 + (W - PW) // stride\n",
        "\n",
        "    # output -- zero array\n",
        "    out = np.zeros((N, C, H_out, W_out))\n",
        "\n",
        "    # loop:\n",
        "    for n in range(N):  # N samples\n",
        "        for c in range(C):  # channels\n",
        "            for h_out in range(H_out):  # output height\n",
        "                for w_out in range(W_out):  # output width\n",
        "                    initial_h = h_out * stride\n",
        "                    initial_w = w_out * stride\n",
        "                    end_h = initial_h + PH\n",
        "                    end_w = initial_w + PW\n",
        "\n",
        "                    # find max\n",
        "                    max_value = -np.inf  # max = very small number\n",
        "                    for i in range(initial_h, end_h):  # pooling region height\n",
        "                        for j in range(initial_w, end_w):  # pooling region width\n",
        "                            if x[n, c, i, j] > max_value:\n",
        "                                max_value = x[n, c, i, j]\n",
        "\n",
        "                    out[n, c, h_out, w_out] = max_value\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, stride, PH, PW)\n",
        "    return out, cache"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JNpYSV8EFF3"
      },
      "source": [
        "### 1.3.2 Pooling Feedforward Check\n",
        "\n",
        "You should observe ~$10^{-8}$ difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSlsvDgEEFgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff094bf5-9f21-47dd-ea35-e06b15016ff9"
      },
      "source": [
        "x_shape = (2, 3, 4, 4)\n",
        "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
        "stride, PH, PW = (2, 2, 2)\n",
        "\n",
        "out, _ = max_pool_forward_naive(x, stride, PH, PW)\n",
        "\n",
        "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
        "                          [-0.20421053, -0.18947368]],\n",
        "                         [[-0.14526316, -0.13052632],\n",
        "                          [-0.08631579, -0.07157895]],\n",
        "                         [[-0.02736842, -0.01263158],\n",
        "                          [ 0.03157895,  0.04631579]]],\n",
        "                        [[[ 0.09052632,  0.10526316],\n",
        "                          [ 0.14947368,  0.16421053]],\n",
        "                         [[ 0.20842105,  0.22315789],\n",
        "                          [ 0.26736842,  0.28210526]],\n",
        "                         [[ 0.32631579,  0.34105263],\n",
        "                          [ 0.38526316,  0.4       ]]]])\n",
        "\n",
        "# Compare your output with ours. Difference should be on the order of e-8.\n",
        "print('Testing max_pool_forward_naive function:')\n",
        "print('difference: ', rel_error(out, correct_out))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing max_pool_forward_naive function:\n",
            "difference:  4.1666665157267834e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_d7rzkMWb-0"
      },
      "source": [
        "### 1.3.3 Pooling Gradient\n",
        "\n",
        "Again, **no** use of vector/matrix multiplications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzVNQ60RWeTR"
      },
      "source": [
        "def max_pool_backward_naive(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a max-pooling layer.\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives\n",
        "    - cache: A tuple of (x, stride, PH, PW) as in the forward pass.\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the max-pooling backward pass                           #\n",
        "    ###########################################################################\n",
        "\n",
        "\n",
        "    x, stride, PH, PW = cache\n",
        "    N, C, H, W = x.shape\n",
        "    H_out, W_out = dout.shape[2], dout.shape[3]\n",
        "\n",
        "    dx = np.zeros_like(x)\n",
        "\n",
        "    # gradient loop:\n",
        "    for n in range(N):  # sample\n",
        "        for c in range(C):  # channel\n",
        "            for h_out in range(H_out):  # output height\n",
        "                for w_out in range(W_out):  # output width\n",
        "                    initial_h = h_out * stride\n",
        "                    initial_w = w_out * stride\n",
        "                    end_h = initial_h + PH\n",
        "                    end_w = initial_w + PW\n",
        "\n",
        "                    # cur pooling region\n",
        "                    max_val = -np.inf # value = -inf\n",
        "                    max_pos = (0, 0)\n",
        "                    for i in range(initial_h, end_h):\n",
        "                        for j in range(initial_w, end_w):\n",
        "                            if x[n, c, i, j] > max_val:\n",
        "                                max_val = x[n, c, i, j]\n",
        "                                max_pos = (i, j)\n",
        "\n",
        "                    # Distribute the gradient from dout to the max location\n",
        "                    dx[n, c, max_pos[0], max_pos[1]] += dout[n, c, h_out, w_out]\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIebBQp0Ejzt"
      },
      "source": [
        "### 1.3.4 Pooling Gradient Check\n",
        "\n",
        "You should observe ~$10^{-12}$ difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rsbl6GlEmF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd6a4ee-3f9c-4f48-b012-74909db37be0"
      },
      "source": [
        "np.random.seed(231)\n",
        "x = np.random.randn(3, 2, 8, 8)\n",
        "dout = np.random.randn(3, 2, 4, 4)\n",
        "stride, PH, PW = (2, 2, 2)\n",
        "\n",
        "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, stride, PH, PW)[0], x, dout)\n",
        "\n",
        "out, cache = max_pool_forward_naive(x, stride, PH, PW)\n",
        "dx = max_pool_backward_naive(dout, cache)\n",
        "\n",
        "# Your error should be on the order of e-12\n",
        "print('Testing max_pool_backward_naive function:')\n",
        "print('dx error: ', rel_error(dx, dx_num))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing max_pool_backward_naive function:\n",
            "dx error:  3.27562514223145e-12\n"
          ]
        }
      ]
    }
  ]
}